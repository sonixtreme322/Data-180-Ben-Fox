#Yes it makes sense that these documents are assigned to topics 1 and 2 accordingly because they have a large gamma.
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
#library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
# Chunk 3
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
sort(unique(news$year))
# Chunk 5
charVector <- news$headline_text
head(charVector)
# Chunk 6
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
# Chunk 7
#Make all text lowercase
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
#Remove punctuation
wordCorpus <- tm_map(wordCorpus, removePunctuation)
#Remove numbers
wordCorpus <- tm_map(wordCorpus, removeNumbers)
#Remove stopwords
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus[["1"]][["content"]]
# Chunk 8
tdm <- TermDocumentMatrix(wordCorpus)
# Chunk 9
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts<- sort(wordCounts, decreasing=TRUE)
head(wordCounts,10)
# Chunk 10
barplot(wordCounts[wordCounts>50],las=2,cex.names=0.75)
# Chunk 11
totalWords <- sum(wordCounts)
#positive words
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP !=0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP>20], las=2,cex.names=0.75)
sum(matchedP)/totalWords
#3.7% positive words
#negative words
matchedN <- match(names(wordCounts), negWords, nomatch = 0)
matchedN <- matchedN !=0
matchedN <- wordCounts[matchedN]
barplot(matchedN[matchedN>20], las=2,cex.names=0.75)
sum(matchedN)/totalWords
#7.8% negative words
# Chunk 12
library(dplyr)
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
# Chunk 13
library(ggplot2)
ggplot(news, aes(x=factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() +theme(axis.text=element_text(size=4,angle=90))
# Chunk 14
library("quanteda")
# Chunk 15
mostFreq <-termFreq(charVector, control = list(removePunctuation=TRUE, stopwords =TRUE))
sort(mostFreq, decreasing = TRUE)[1:20]
# Chunk 16
library(tokenizers)
tokenize_words(charVector)
tokenize_ngrams(charVector, n=2)
library(ngram)
words <- paste(unlist(charVector), collapse = "")
ng <- ngram(words, n=2)
head(get.phrasetable(ng), 20)
# Chunk 17
newscorpus <- corpus(charVector)
paras <- corpus_reshape(newscorpus, to="paragraphs")
# Chunk 18
news_dtm <- dfm(paras, stem=TRUE, remove_punct=TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove=c(stopwords("english")))
news_dtm <- dfm_remove(news_dtm,c('sa','=','nt','qld','nsw','abc','news'))
news_dtm <-dfm_trim(news_dtm, min_termfreq=50)
head(news_dtm)
# Chunk 19
library("quanteda.textplots")
textplot_wordcloud(news_dtm)
#Surprised: "australia", "perth", "canberra", Not Surprised: "man", "police", "new", "say"
# Chunk 20
library("topicmodels")
library('tidytext')
# Chunk 21
news_topics <- convert(news_dtm, to = "topicmodels")
topic_model <- LDA(news_topics, method = "VEM", control=list(seed=1234), k=8)
terms(topic_model,8)
# Chunk 22
#Beta is the probability that a given term appears in a particular topic
#Higher probability terms "define" the topic best
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
#can show, adds up to 1 for each topic
#tidy_topics %>% group_by(topic) %>% summarize(sum_beta = sum(beta))
# no need to understand this as much, can copy paste when needed.
news_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>% #cool func, gets the max n for each topic group
ungroup() %>% #to get the tibble without group tag
arrange(topic, -beta) #sort by topic, beta decreasing
news_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>% #this hack is to order for facet
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend= FALSE) +
facet_wrap(~topic, scales = "free") + # scales="free" allows x-y scales to be free
scale_y_recordered() #used in combo with reorder-within
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
#library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
# Chunk 3
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
sort(unique(news$year))
# Chunk 5
charVector <- news$headline_text
head(charVector)
# Chunk 6
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
# Chunk 7
#Make all text lowercase
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
#Remove punctuation
wordCorpus <- tm_map(wordCorpus, removePunctuation)
#Remove numbers
wordCorpus <- tm_map(wordCorpus, removeNumbers)
#Remove stopwords
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus[["1"]][["content"]]
# Chunk 8
tdm <- TermDocumentMatrix(wordCorpus)
# Chunk 9
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts<- sort(wordCounts, decreasing=TRUE)
head(wordCounts,10)
# Chunk 10
barplot(wordCounts[wordCounts>50],las=2,cex.names=0.75)
# Chunk 11
totalWords <- sum(wordCounts)
#positive words
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP !=0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP>20], las=2,cex.names=0.75)
sum(matchedP)/totalWords
#3.7% positive words
#negative words
matchedN <- match(names(wordCounts), negWords, nomatch = 0)
matchedN <- matchedN !=0
matchedN <- wordCounts[matchedN]
barplot(matchedN[matchedN>20], las=2,cex.names=0.75)
sum(matchedN)/totalWords
#7.8% negative words
# Chunk 12
library(dplyr)
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
# Chunk 13
library(ggplot2)
ggplot(news, aes(x=factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() +theme(axis.text=element_text(size=4,angle=90))
# Chunk 14
library("quanteda")
# Chunk 15
mostFreq <-termFreq(charVector, control = list(removePunctuation=TRUE, stopwords =TRUE))
sort(mostFreq, decreasing = TRUE)[1:20]
# Chunk 16
library(tokenizers)
tokenize_words(charVector)
tokenize_ngrams(charVector, n=2)
library(ngram)
words <- paste(unlist(charVector), collapse = "")
ng <- ngram(words, n=2)
head(get.phrasetable(ng), 20)
# Chunk 17
newscorpus <- corpus(charVector)
paras <- corpus_reshape(newscorpus, to="paragraphs")
# Chunk 18
news_dtm <- dfm(paras, stem=TRUE, remove_punct=TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove=c(stopwords("english")))
news_dtm <- dfm_remove(news_dtm,c('sa','=','nt','qld','nsw','abc','news'))
news_dtm <-dfm_trim(news_dtm, min_termfreq=50)
head(news_dtm)
# Chunk 19
library("quanteda.textplots")
textplot_wordcloud(news_dtm)
#Surprised: "australia", "perth", "canberra", Not Surprised: "man", "police", "new", "say"
# Chunk 20
library("topicmodels")
library('tidytext')
# Chunk 21
news_topics <- convert(news_dtm, to = "topicmodels")
topic_model <- LDA(news_topics, method = "VEM", control=list(seed=1234), k=8)
terms(topic_model,8)
# Chunk 22
#Beta is the probability that a given term appears in a particular topic
#Higher probability terms "define" the topic best
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
#can show, adds up to 1 for each topic
#tidy_topics %>% group_by(topic) %>% summarize(sum_beta = sum(beta))
# no need to understand this as much, can copy paste when needed.
news_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>% #cool func, gets the max n for each topic group
ungroup() %>% #to get the tibble without group tag
arrange(topic, -beta) #sort by topic, beta decreasing
news_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>% #this hack is to order for facet
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend= FALSE) +
facet_wrap(~topic, scales = "free") + # scales="free" allows x-y scales to be free
scale_y_recordered() #used in combo with reorder-within
library(tokenizers)
tokenize_words(charVector)
tokenize_ngrams(charVector, n=2)
library(ngram)
words <- paste(unlist(charVector), collapse = "")
ng <- ngram(words, n=2)
head(get.phrasetable(ng), 20)
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
# Chunk 3
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
line_count = nrow(news[, "year", drop = FALSE])
c('Number of lines in year column:', line_count)
# This code gives the number of lines within the year column.
range = max(news$year) - min(news$year)
range
# This is the actual the range of years
# Chunk 5
charVector = news$'headline_text'
head(charVector, 6)
# Chunk 6
wordVector = VectorSource(charVector)
wordCorpus = Corpus(wordVector)
# Chunk 7
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
print(content(wordCorpus[1]))
# Chunk 8
# A term document matrix is a collection of documents where rows represent words or terms while columns correspond to documents.
tmd = TermDocumentMatrix(news)
head(tmd)
# Chunk 9
m = as.matrix(tmd)
wordCounts = rowSums(m)
sort_Word_count = sort(wordCounts, decreasing = TRUE)
head(sort_Word_count,10)
totalWords <- sum(wordCounts)
totalWords
# found of the total number of words for question 8.
# Chunk 10
counting_words = wordCounts[wordCounts >= 50]
barplot(wordCounts, las = 2, cex.names = 0.75, main = 'Top words in headlines', xlab = 'Words', ylab = 'Frequency')
# Chunk 11
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- wordCounts[matchedP != 0]
barplot(matchedP,las=2,cex.names=0.75)
sum(matchedP)/totalWords
matchedneg <- match(names(wordCounts), negWords, nomatch = 0)
matchedneg <- wordCounts[matchedneg != 0]
barplot(matchedneg,las=2,cex.names=0.75)
sum(matchedneg)/totalWords
# The percentage of positive words in the news headlines comes out to 1.5%.
# The percentage of negative words in the news headlines comes out to 3%.
# Chunk 12
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
# Chunk 13
library(ggplot2)
ggplot(news, aes(x=factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() +
theme(axis.text.x = element_text(size = 4, angle = 90)) +
labs(x = 'Year/Month', y = 'Number of Articles Released', title = 'Frequency of    Articles Over Time')
# One sees that there is an overall downward trend of articles in the past few years.  Overall, it seems as though not as many articles are posted as there were in previous years.
# Chunk 14
library("quanteda")
install.packages("corpus")
library("corpus")
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
# Chunk 3
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
line_count = nrow(news[, "year", drop = FALSE])
c('Number of lines in year column:', line_count)
# This code gives the number of lines within the year column.
range = max(news$year) - min(news$year)
range
# This is the actual the range of years
# Chunk 5
charVector = news$'headline_text'
head(charVector, 6)
# Chunk 6
wordVector = VectorSource(charVector)
wordCorpus = Corpus(wordVector)
# Chunk 7
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
print(content(wordCorpus[1]))
# Chunk 8
# A term document matrix is a collection of documents where rows represent words or terms while columns correspond to documents.
tmd = TermDocumentMatrix(news)
head(tmd)
# Chunk 9
m = as.matrix(tmd)
wordCounts = rowSums(m)
sort_Word_count = sort(wordCounts, decreasing = TRUE)
head(sort_Word_count,10)
totalWords <- sum(wordCounts)
totalWords
# found of the total number of words for question 8.
# Chunk 10
counting_words = wordCounts[wordCounts >= 50]
barplot(wordCounts, las = 2, cex.names = 0.75, main = 'Top words in headlines', xlab = 'Words', ylab = 'Frequency')
# Chunk 11
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- wordCounts[matchedP != 0]
barplot(matchedP,las=2,cex.names=0.75)
sum(matchedP)/totalWords
matchedneg <- match(names(wordCounts), negWords, nomatch = 0)
matchedneg <- wordCounts[matchedneg != 0]
barplot(matchedneg,las=2,cex.names=0.75)
sum(matchedneg)/totalWords
# The percentage of positive words in the news headlines comes out to 1.5%.
# The percentage of negative words in the news headlines comes out to 3%.
# Chunk 12
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
# Chunk 13
library(ggplot2)
ggplot(news, aes(x=factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() +
theme(axis.text.x = element_text(size = 4, angle = 90)) +
labs(x = 'Year/Month', y = 'Number of Articles Released', title = 'Frequency of    Articles Over Time')
# One sees that there is an overall downward trend of articles in the past few years.  Overall, it seems as though not as many articles are posted as there were in previous years.
# Chunk 14
library("quanteda")
#install.packages("corpus")
#library("corpus")
# Chunk 15
x = termFreq(charVector)
sort(x, decreasing = TRUE)[1:20]
# Chunk 16
term_stats(charVector, ngrams = 2,filter = text_filter(drop_punct = TRUE, drop_symbol = TRUE, drop = stopwords_en))
newscorpus = corpus(charVector)
paragraph_corpus = corpus_reshape(newscorpus, to="paragraphs")
newscorpus = corpus(charVector)
paragraph_corpus = corpus_reshape(newscorpus, to="paragraphs")
newscorpus = corpus(charVector)
paragraph_corpus = corpus_reshape(newscorpus, to="paragraphs")
news_dtm = dfm(paragraph_corpus, stem = TRUE,
remove = c(punt=TRUE, numbers = TRUE, symbols = TRUE, stopwords("en")))
news_dtm = dfm_remove(news_dtm, c('s','?','?','thi'))
news_dtm = dfm_trim(news_dtm, min_termfreq = 50)
head(news_dtm, 6)
library("quanteda.textplots")
q14 = textplot_wordcloud(news_dtm,color = rev(RColorBrewer::brewer.pal(10, 'RdBu')))
q14
# I am not surprised to see words like report, man, charg, murder, and more.
# I am surprised to see face, australia, sydney, melbourne, and more.
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
# Chunk 3
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
print(news)
#There is 18 years of data loaded in (2003-2021)
# Chunk 5
charVector <- news$headline_text
head(charVector, 6)
# Chunk 6
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
# Chunk 7
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus$content[1]
# Chunk 8
#A term document matrix depicts the frequency of terms that occur in a document or multiple documents.
tdm <- TermDocumentMatrix(wordCorpus)
# Chunk 9
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing = TRUE)
head(wordCounts, 10)
# Chunk 10
many_word_counts <- wordCounts[wordCounts >= 50]
barplot(many_word_counts, las = 2, cex.names = 0.75, main = "Words That Showed Up 50 Times", xlab = "Words", ylab = "Frequency")
# Chunk 11
totalWords <- sum(wordCounts)
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP >= 20], las = 2, cex.names = 0.75, xlab = "Words", ylab = "Frequency")
sum(matchedN)/totalWords
matchedN <- match(names(wordCounts), negWords, nomatch = 0)
matchedN <- matchedN != 0
matchedN <- wordCounts[matchedN]
barplot(matchedN[matchedN >= 20], las = 2, cex.names = 0.75, xlab = "Words", ylab = "Frequency")
sum(matchedN)/totalWords
# Chunk 12
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
print(news)
# Chunk 13
library(ggplot2)
library(dplyr)
ggplot(news, aes(x = factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() + labs(title = "Frequency of Articles", x = "Year/Month", y = "Number of Articles") + theme(axis.text=element_text(size=4,angle=90))
# Chunk 14
install.packages("quanteda")
install.packages("corpus")
library("quanteda")
library('corpus')
install.packages("quanteda")
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
top_anthems <- tidy_anthems %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
top_anthems %>%
mutate(document = reorder_within(document, gamma, topic)) %>%
ggplot(aes(gamma, document, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
scale_y_reordered()
library(dplyr)
library(ggplot2)
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
anthem_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
anthem_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") + scale_y_reordered()
#The first headline group appears to include personal disasters and crime with words like death, charge, death, and hospital. The word police shows that a crime must have taken place. 3 appears to be political, as the words council, new, power govern, and nation are at the top as if there was an election of some sort. 6 seems to be about manslaughter, but the word win and interview seem to make this not so traumatic. 8 seems to deal again with the police, maybe about "inquiring" about a suspect or disaster.
